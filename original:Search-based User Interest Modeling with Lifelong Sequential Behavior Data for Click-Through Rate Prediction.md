## ABSTRACT
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
Rich user behavior data has been proven to be of great value for click-through rate prediction tasks, especially in industrial applications such as recommender systems and online advertising. Both industry and academy have paid much attention to this topic and propose different approaches to modeling with long sequential user behavior data. Among them, memory network based model MIMN[8] proposed by Alibaba, achieves SOTA with the co-design of both learning algorithm and serving system. MIMN is the first industrial solution that can model sequential user behavior data with length scaling up to 1000. However, MIMN fails to precisely capture user interests given a specific candidate item when the length of user behavior sequence increases further, say, by 10 times or more. This challenge exists widely in previously proposed approaches.    
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
In this paper, we tackle this problem by designing a new modeling paradigm, which we name as Search-based Interest Model (SIM). SIM extracts user interests with two cascaded search units: (i) General Search Unit (GSU) acts as a general search from the raw and arbitrary long sequential behavior data, with query information from candidate item, and gets a Sub user Behavior Sequence (SBS) which is relevant to candidate item; (ii) Exact Search Unit (ESU) models the precise relationship between candidate item and SBS. This cascaded search paradigm enables SIM with a better ability to model lifelong sequential behavior data in both scalability and accuracy. Apart from the learning algorithm, we also introduce our hands-on experience on how to implement SIM in large scale industrial systems. Since 2019, SIM has been deployed in the display advertising system in Alibaba, bringing 7.1% CTR and 4.4% RPM lift, which is significant to the business. Serving the main traffic in our real system now, SIM models sequential user behavior data with maximum length reaching up to 54000, pushing SOTA to 54x.


## 1 INTRODUCTION

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
Click-Through Rate (CTR) prediction modeling plays a critical role in industrial applications such as recommender systems and online advertising. Due to the rapid growth of user historical behavior data, user interest modeling, which focuses on learning the intent representation of user interest, has been widely introduced in the CTR prediction model [2, 8, 19, 20]. However, most of the proposed approaches can only model sequential user behavior data with length scaling up to hundreds, limited by the burden of computation and storage in real online systems [19, 20]. Rich user behavior data is proven to be of great value [8]. For example, 23% of users in Taobao, one of the world’s leading e-commerce site, click with more than 1000 products in last 5 months[8, 10]. How to design a feasible solution to model the long sequential user behavior data has been an open and hot topic, attracting researchers from both industry and academy.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
A branch of research, which borrows ideas from the area of NLP, proposes to model long sequential user behavior data with mem- ory network and makes some breakthroughs. MIMN[8] proposed by Alibaba, is one typical work, which achieves SOTA with the co-design of both learning algorithm and serving system. MIMN is the first industrial solution which can model sequential user behavior data with length scaling up to 1000. Specifically, MIMN incrementally embeds diverse interest of one user into a fixed size memory matrix which will be updated by each new behavior. In that way, the computation of user modeling is decoupled from CTR prediction. Thus for online serving, latency will not be a problem and the storage cost depends on the size of the memory matrix which is much less than the raw behavior sequence. A similar idea can be found in long-term interest modeling[10]. However, it is still challenging for memory network based approaches to model arbitrary long sequential data. Practically, we find that MIMN fails to precisely capture user interest given a specific candidate item when the length of user behavior sequence increases further, say, up to 10000 or more. This is because encoding all user historical behaviors into a fixed size memory matrix causes massive noise to be contained in the memory units.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
On the other hand, as pointed out in the previous work of DIN[20], the interest of one user is diverse and varies when facing different candidate items. The key idea of DIN is searching the ef- fective information from user behaviors to model special interest of user, facing different candidate items. In this way, we can tackle the challenge of encoding all user interest into fixed-size parameters. DIN does bring a big improvement for CTR modeling with user behavior data. But the searching formula of DIN costs an unac- ceptable computation and storage facing the long sequential user behavior data as we mentioned above. So, can we apply a similar search trick and design a more efficient way to extract knowledge from the long sequential user behavior data?

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
In this paper, we tackle the challenge by designing a new modeling paradigm, which we name as Search-based Interest Model (SIM). SIM adopts the idea of DIN [20] and captures only relevant user interest with respect to specific candidate items. In SIM, user interest is extracted with two cascaded search units:(i) General Search Unit (GSU) acts as a general search from the raw and arbi- trary long sequential behavior data, with query information from candidate item, and gets a Sub user Behavior Sequence (SBS) which is relevant to candidate item. In order to meet the strict limitation of latency and computation resources, general but effective methods are utilized in the GSU. To our experience, the length of SBS can be cut down to hundreds and most of the noise information in raw long sequential behavior data could be filtered. (ii) Exact Search Unit (ESU) models the precise relationship between the candidate item and SBS. Here we can easily apply similar methods proposed by DIN[20] or DIEN[19].

The main contributions of this work are summarized as follows:
 - We propose a new paradigm SIM for modeling long sequential user behavior data. The design of a cascaded two-stage search mechanism enables SIM with a better ability to model lifelong sequential behavior data in both scalability and ac- curacy.
 - We introduce our hands-on experience of implementing SIM in large scale industrial systems. Since 2019, SIM has been deployed in the display advertising system in Alibaba, bringing 7.1% CTR and 4.4% RPM lift. Now, SIM is serving the main traffic. 
 - We push the maximum length for modeling with long sequential user behavior data up to 54000, 54x larger than MIMN, the published SOTA industry solution for this task.

## 2  RELATED WORK
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
__User Interest Model.__ Deep learning based methods have achieved great success in CTR prediction task[1, 11, 18]. In early age, most pioneer works[1, 4, 7, 9, 15] use a deep neural network to capture interactions between features from different fields so that engineers could get rid of boring feature engineering works. Recently, a series of works, which we called User Interest Model, focus on learning the representation of latent user interest from historical behaviors, using different neural network architecture such as CNN[14, 17], RNN[5, 19], Transformer[3, 13] and Capsule[6], etc. DIN[20] emphasizes that user interest are diverse and an attention mechanism is introduced in DIN to capture users’ diverse interest on the differ- ent target items. DIEN[19] points out that the temporal relationship between historical behaviors matters for modeling users’ drifting interest. An interest extraction layer based on GRU with auxiliary loss is designed in DIEN. MIND[6] argues that using a single vector to represent one user is insufficient to capture the varying nature of the user’s interest. Capsule network and dynamic routing method are introduced in MIND to learn the representation of user interest as multiple vectors. Moreover, inspired by the success of the self-attention architecture in the tasks of sequence to sequence learning, Transformer is introduced in [3] to model user cross-session and in-session interest.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
__Long-term User Interest.__ [8] shows that considering long- term historical behavior sequences in the user interest model can significantly improve CTR model performance. Although longer user behavior sequences bring in more useful information for user interest modeling, it extremely burdens the latency and storage of an online serving system and contains massive noise for point-wise CTR prediction at the same time. A series of works focus on tackling the challenge in long-term user interest modeling, which usually learns user interest representation based on historical behavior sequences with extremely large length even lifelong. [10] proposes a Hierarchical Periodic Memory Network for lifelong sequential modeling with personalized memorization of sequential patterns for each user. [16] choose an attention-based framework to combine users’ long-term and short-term preferences. And they adopt the attentive Asymmetric-SVD paradigm to model long-term interest. A memory-based architecture named MIMN is proposed in [8], which embedded user long-term interest into fixed-sized memory network to solve the problem of large storage of user behavior data. And a UIC module is designed to record the new user behaviors incrementally to deal with the latency limitation. But MIMN abandons the information from the target item in the memory network which has been proved to be important for user interest modeling.

## 3 SEARCH-BASED INTEREST MODEL
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
It has been proven to be effective by modeling user behavior data for CTR prediction modeling. Typically, attention-based CTR models, such as DIN[20] and DIEN[19], design complex model structure and involve attention mechanism to capture user diverse interest by searching effective knowledge from user behavior sequence, with inputs from different candidate items. But in a real-world system, these models can only handle short-term behavior sequence data, of which the length is usually less than 150. On the other hand, the long-term user behavior data is valuable, and modeling the long- term interest of users may bring more diverse recommendation results for users. It seems we are on the horns of a dilemma: we cannot handle the valuable life-long user behavior data with the effective but complex methods in a real-world system. To tackle this challenge, we propose a new modeling paradigm, which is named as Search-based Interest Model (SIM). SIM follows a two-stage search strategy and can handle long user behavior sequences in an efficient way. In this section, we will introduce the overall workflow of SIM first and then introduce the two proposed search units in detail

### 3.1 Overall Workflow

The overall workflow of SIM is shown in Figure 1. SIM follows a cascaded two-stage search strategy with two corresponding units: the General Search Unit (GSU) and the Exact Search Unit (ESU).

**In the first stage**, we utilize General Search Unit (GSU) to seek top-K relevant sub behavior sequences from original long-term behavior sequences with sub-linear time complexity. Here K is generally much shorter than the original length of behavior sequences. An efficient search method can be conducted if the relevant behaviors can be searched under the limitation of time and computation resources. In section 3.2 we provide two straightforward implementations of GSU: soft-search and hard-search. GSU takes a general but effective strategy to cut off the length of raw sequential behaviors to meet the strict limitation of time and computation resources.
Meanwhile, the massive noise that exists in the long-term user behavior sequence, which may undermine user interest modeling, can be filtered by the search strategy in the first stage.

**In the second stage**, the Exact Search Unit (ESU), which takes the filtered sub-sequential user behaviors as input, is introduced to further capture the precise user interest. Here a sophisticated model with complex architecture can be applied, such as DIN[20] and DIEN[19], as the length of long-term behaviors has been reduced to hundreds.

Note that although we introduce the two stages separately, actually they are trained together.


### 3.2 General Search Unit

Given a candidate item (the target item to be scored by CTR model), only a part of user behaviors are valuable. This part of user behaviors are closely related to final user decision. Picking out these relevant user behaviors is helpful in user interest modeling. However, using the whole user behavior sequence to directly model the user interest will bring enormous resource usage and response latency, which is usually unacceptable in practical applications. To this end, we propose a general search unit to cut down the input number of user behaviors in user interest modeling. Here we introduce two kinds of general search unit: hard-search and soft-search.

Given the list of user behaviors B = [b₁; b₂; ⋯ ; bₜ], where bᵢ is the i-th user behavior and T is the length of user behaviors. The general search unit can calculate relevant score rᵢ for each behavior bᵢ with the candidate item and then select the Top-K relevant behaviors with score rᵢ as sub behavior sequence B*. The difference between hard-search and soft-search is the formulation of relevant score rᵢ:

![image.png](https://s2.loli.net/2024/11/11/AGxpevWoZdaMUPm.jpg)

**Hard-search**. The hard-search model is non-parametric. Only behavior belongs to the same category as the candidate item will be selected and aggregated as a sub behavior sequence to be sent to the exact search unit. Here Cₐ and Cᵢ denote the categories of target item and the i-th behavior bᵢ that belong to correspondingly. Hard-search is straightforward but later in section 4 we will show how it is quite suitable for online serving.

**Soft-search**. In the soft-search model, bᵢ is first encoded as one-hot vector and then embedded into low-dimensional vectors E = [e₁; e₂; ⋯ ; eₜ], as shown in Figure 1. Wᵦ and Wᵥ are the parameters of weight. eₐ and eᵢ denote the embedding vectors of target item and i-th behavior bᵢ, respectively. To further speed up the top-K search over ten thousands length of user behaviors, sublinear time maximum inner product search method ALSH[12] is conducted based on the embedding vectors E to search the related top-K behaviors with target item. With the well-trained embedding and Maximum Inner Product Search (MIPS) method, over ten thousands of user behaviors could be reduced to hundreds.

It should be noticed that distributions of long-term and short-term data are different. Thus, directly using the parameters learned from short-term user interest modeling in soft-search model may mislead the long-term user interest modeling. In this paper, the parameters of soft-search model is trained under an auxiliary CTR prediction task based on long-term behavior data, illustrated as soft search training in the left of Figure 1. The behaviors representation Uᵣ is obtained by multiplying the rᵢ and eᵢ:

![image.png](https://s2.loli.net/2024/11/11/9SzB3TkaOn2IKAV.jpg)

The behaviors representation Uᵣ and the target vector eₐ are then concatenated as the input of following MLP (Multi-Layer Perception). Note that if the user behavior grows to a certain extent, it is impossible to directly fed the whole user behaviors into the model. In that situation, one can randomly sample sets of sub-sequence from the long sequential user behaviors, which still follows the same distribution of the original one.

